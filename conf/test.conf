# To save typing, this file offers a way to associate
# default config files (-c) and datase directories (-D) with expnames 
config {
    # expname = config_file_path
    # path should be under <project root>
    sn64 = conf/exp/sn64.conf
    sn64_unseen = conf/exp/sn64_unseen.conf
    srn_chair = conf/exp/srn.conf
    srn_car = conf/exp/srn.conf
    dtu = conf/exp/dtu.conf
    multi_obj = conf/exp/multi_obj.conf
}

datadir {
    # expname = data directory
}

conf/default_mv.conf
# Main multiview supported config
include required("default.conf")
model {
    # MLP architecture
    # Adapted for multiview
    # Possibly too big
    mlp_coarse {
        type= resnet
        n_blocks= 5
        d_hidden= 512
        # Combine after 3rd layer by average
        combine_layer= 3
        combine_type= average
    }
    
    mlp_fine {
        type= resnet
        n_blocks= 5
        d_hidden= 512
        combine_layer= 3
        combine_type= average
    }
}

# Single-view only base model
# (Not used in experiments; resnet_fine_mv.conf inherits)
model {
    # Condition on local encoder
    use_encoder = True

    # Condition also on a global encoder?
    use_global_encoder = False
    
    # Use xyz input instead of just z
    # (didn't ablate)
    use_xyz = True
    
    # Canonical space xyz (default view space)
    canon_xyz = False

    # Positional encoding
    use_code = True
    code {
        num_freqs = 6
        freq_factor = 1.5
        include_input = True
    }

    # View directions
    use_viewdirs = True
    # Apply pos. enc. to viewdirs?
    use_code_viewdirs = False

    # MLP architecture
    mlp_coarse {
        type = resnet  # Can change to mlp
        n_blocks = 3
        d_hidden = 512
    }
    mlp_fine {
        type = resnet
        n_blocks = 3
        d_hidden = 512
    }

    # Encoder architecture
    encoder {
        backbone = resnet34
        pretrained = True
        num_layers = 4
    }
}
renderer {
    n_coarse = 64
    n_fine = 32
    # Try using expected depth sample
    n_fine_depth = 16
    # Noise to add to depth sample
    depth_std = 0.01
    # Decay schedule, not used
    sched = []
    # White background color (false : black)
    white_bkgd = True
}
loss {
    # RGB losses coarse/fine
    rgb {
        use_l1 = False
    }
    rgb_fine {
        use_l1 = False
    }
    # Alpha regularization (disabled in final version)
    alpha {
        # lambda_alpha = 0.0001
        lambda_alpha = 0.0
        clamp_alpha = 100
        init_epoch = 5
    }
    # Coarse/fine weighting (nerf = equal)
    lambda_coarse = 1.0  # loss = lambda_coarse * loss_coarse + loss_fine
    lambda_fine = 1.0  # loss = lambda_coarse * loss_coarse + loss_fine
}
train {
    # Training 
    print_interval = 2
    save_interval = 50
    vis_interval = 100
    eval_interval = 50

    # Accumulating gradients. Not really recommended.
    # 1 = disable
    accu_grad = 1

    # Number of times to repeat dataset per 'epoch'
    # Useful if dataset is extremely small, like DTU
    num_epoch_repeats = 1
}